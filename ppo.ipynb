{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "from tetris_gymnasium.envs.tetris import Tetris\n",
    "import gymnasium as gym\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network for Actor (Policy) and Critic (Value) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy and value model\n",
    "class ActorCriticNetwork(nn.Module):\n",
    "    def __init__(self, obs_space_size, action_space_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.shared_layers = nn.Sequential(\n",
    "            nn.Linear(obs_space_size, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "\n",
    "        self.policy_layers = nn.Sequential(\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, action_space_size)\n",
    "        )\n",
    "        \n",
    "\n",
    "        self.value_layers = nn.Sequential(\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "    \n",
    "    def value(self, obs):\n",
    "        z = self.shared_layers(obs)\n",
    "        value = self.value_layers(z)\n",
    "        return value\n",
    "        \n",
    "    def policy(self, obs):\n",
    "        z = self.shared_layers(obs)\n",
    "        policy_logits = self.policy_layers(z)\n",
    "        return policy_logits\n",
    "\n",
    "    def forward(self, obs):\n",
    "        z = self.shared_layers(obs)\n",
    "        policy_logits = self.policy_layers(z)\n",
    "        value = self.value_layers(z)\n",
    "        return policy_logits, value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOTrainer():\n",
    "  def __init__(self,\n",
    "              actor_critic,\n",
    "              ppo_clip_val=0.4,\n",
    "              target_kl_div=0.01,\n",
    "              max_policy_train_iters=80,\n",
    "              value_train_iters=80,\n",
    "              policy_lr=3e-4,\n",
    "              value_lr=3e-4):\n",
    "    self.ac = actor_critic\n",
    "    self.ppo_clip_val = ppo_clip_val\n",
    "    self.target_kl_div = target_kl_div\n",
    "    self.max_policy_train_iters = max_policy_train_iters\n",
    "    self.value_train_iters = value_train_iters\n",
    "\n",
    "    policy_params = list(self.ac.shared_layers.parameters()) + \\\n",
    "        list(self.ac.policy_layers.parameters())\n",
    "    self.policy_optim = optim.Adam(policy_params, lr=policy_lr)\n",
    "\n",
    "    value_params = list(self.ac.shared_layers.parameters()) + \\\n",
    "        list(self.ac.value_layers.parameters())\n",
    "    self.value_optim = optim.Adam(value_params, lr=value_lr)\n",
    "\n",
    "  def train_policy(self, obs, acts, old_log_probs, gaes):\n",
    "    for _ in range(self.max_policy_train_iters):\n",
    "      self.policy_optim.zero_grad()\n",
    "\n",
    "      new_logits = self.ac.policy(obs)\n",
    "      new_logits = Categorical(logits=new_logits)\n",
    "      new_log_probs = new_logits.log_prob(acts)\n",
    "\n",
    "      policy_ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "      clipped_ratio = policy_ratio.clamp(\n",
    "          1 - self.ppo_clip_val, 1 + self.ppo_clip_val)\n",
    "      \n",
    "      clipped_loss = clipped_ratio * gaes\n",
    "      full_loss = policy_ratio * gaes\n",
    "      policy_loss = -torch.min(full_loss, clipped_loss).mean()\n",
    "\n",
    "      policy_loss.backward()\n",
    "      self.policy_optim.step()\n",
    "\n",
    "      kl_div = (old_log_probs - new_log_probs).mean()\n",
    "      if kl_div >= self.target_kl_div:\n",
    "        break\n",
    "\n",
    "  def train_value(self, obs, returns):\n",
    "    for _ in range(self.value_train_iters):\n",
    "      self.value_optim.zero_grad()\n",
    "\n",
    "      values = self.ac.value(obs)\n",
    "      value_loss = (returns - values) ** 2\n",
    "      value_loss = value_loss.mean()\n",
    "\n",
    "      value_loss.backward()\n",
    "      self.value_optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, gamma=0.99):\n",
    "    new_rewards = [float(rewards[-1])]\n",
    "    for i in reversed(range(len(rewards)-1)):\n",
    "        new_rewards.append(float(rewards[i]) + gamma * new_rewards[-1])\n",
    "    return np.array(new_rewards[::-1])\n",
    "\n",
    "def calculate_gaes(rewards, values, gamma=0.99, decay=0.97):\n",
    "    next_values = np.concatenate([values[1:], [0]])\n",
    "    deltas = [rew + gamma * next_val - val for rew, val, next_val in zip(rewards, values, next_values)]\n",
    "\n",
    "    gaes = [deltas[-1]]\n",
    "    for i in reversed(range(len(deltas)-1)):\n",
    "        gaes.append(deltas[i] + decay * gamma * gaes[-1])\n",
    "\n",
    "    return np.array(gaes[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess observation function\n",
    "def preprocess_observation(observation):\n",
    "    board = observation['board'].flatten()\n",
    "    active_tetromino = observation['active_tetromino_mask'].flatten()\n",
    "    holder = observation['holder'].flatten()\n",
    "    queue = observation['queue'].flatten()\n",
    "    processed_observation = np.concatenate((board, active_tetromino, holder, queue))\n",
    "    processed_observation = processed_observation / 9.0\n",
    "    return processed_observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(model, env, max_steps=1000):\n",
    "    ### Create data storage\n",
    "    train_data = [[], [], [], [], []] # obs, act, reward, values, act_log_probs\n",
    "    obs, _ = env.reset()\n",
    "\n",
    "    ep_reward = 0\n",
    "    for _ in range(max_steps):\n",
    "        obs = preprocess_observation(obs)\n",
    "        logits, val = model(torch.tensor([obs], dtype=torch.float32,\n",
    "                                         device=DEVICE))\n",
    "        act_distribution = Categorical(logits=logits)\n",
    "        act = act_distribution.sample()\n",
    "        act_log_prob = act_distribution.log_prob(act).item()\n",
    "\n",
    "        act, val = act.item(), val.item()\n",
    "\n",
    "        next_obs, reward, done, _, _ = env.step(act)\n",
    "\n",
    "        for i, item in enumerate((obs, act, reward, val, act_log_prob)):\n",
    "          train_data[i].append(item)\n",
    "\n",
    "        obs = next_obs\n",
    "        ep_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    train_data = [np.asarray(x) for x in train_data]\n",
    "\n",
    "    ### Do train data filtering\n",
    "    train_data[3] = calculate_gaes(train_data[2], train_data[3])\n",
    "\n",
    "    return train_data, ep_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"tetris_gymnasium/Tetris\")\n",
    "obs, _ = env.reset()\n",
    "obs = preprocess_observation(obs)\n",
    "len(obs)\n",
    "env.action_space.n\n",
    "model = ActorCriticNetwork(len(obs), env.action_space.n)\n",
    "model = model.to(DEVICE)\n",
    "train_data, reward = rollout(model, env) # Test rollout function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training params\n",
    "n_episodes = 15000\n",
    "print_freq = 20\n",
    "\n",
    "ppo = PPOTrainer(\n",
    "    model,\n",
    "    policy_lr = 3e-4,\n",
    "    value_lr = 1e-3,\n",
    "    target_kl_div = 0.02,\n",
    "    max_policy_train_iters = 40,\n",
    "    value_train_iters = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20 | Avg Reward 12.2\n",
      "Episode 40 | Avg Reward 10.4\n",
      "Episode 60 | Avg Reward 10.8\n",
      "Episode 80 | Avg Reward 11.2\n",
      "Episode 100 | Avg Reward 11.3\n",
      "Episode 120 | Avg Reward 10.1\n",
      "Episode 140 | Avg Reward 9.9\n",
      "Episode 160 | Avg Reward 9.8\n",
      "Episode 180 | Avg Reward 10.8\n",
      "Episode 200 | Avg Reward 10.2\n",
      "Episode 220 | Avg Reward 13.9\n",
      "Episode 240 | Avg Reward 12.3\n",
      "Episode 260 | Avg Reward 12.6\n",
      "Episode 280 | Avg Reward 12.1\n",
      "Episode 300 | Avg Reward 10.5\n",
      "Episode 320 | Avg Reward 12.5\n",
      "Episode 340 | Avg Reward 10.5\n",
      "Episode 360 | Avg Reward 13.3\n",
      "Episode 380 | Avg Reward 13.9\n",
      "Episode 400 | Avg Reward 12.1\n",
      "Episode 420 | Avg Reward 12.4\n",
      "Episode 440 | Avg Reward 13.0\n",
      "Episode 460 | Avg Reward 12.2\n",
      "Episode 480 | Avg Reward 11.8\n",
      "Episode 500 | Avg Reward 13.9\n",
      "Episode 520 | Avg Reward 11.9\n",
      "Episode 540 | Avg Reward 10.3\n",
      "Episode 560 | Avg Reward 11.3\n",
      "Episode 580 | Avg Reward 12.3\n",
      "Episode 600 | Avg Reward 12.7\n",
      "Episode 620 | Avg Reward 12.9\n",
      "Episode 640 | Avg Reward 10.7\n",
      "Episode 660 | Avg Reward 10.8\n",
      "Episode 680 | Avg Reward 10.4\n",
      "Episode 700 | Avg Reward 10.4\n",
      "Episode 720 | Avg Reward 10.1\n",
      "Episode 740 | Avg Reward 10.1\n",
      "Episode 760 | Avg Reward 10.2\n",
      "Episode 780 | Avg Reward 10.0\n",
      "Episode 800 | Avg Reward 9.9\n",
      "Episode 820 | Avg Reward 10.5\n",
      "Episode 840 | Avg Reward 9.9\n",
      "Episode 860 | Avg Reward 10.4\n",
      "Episode 880 | Avg Reward 10.2\n",
      "Episode 900 | Avg Reward 10.2\n",
      "Episode 920 | Avg Reward 10.1\n",
      "Episode 940 | Avg Reward 9.8\n",
      "Episode 960 | Avg Reward 10.0\n",
      "Episode 980 | Avg Reward 9.8\n",
      "Episode 1000 | Avg Reward 10.1\n",
      "Episode 1020 | Avg Reward 10.0\n",
      "Episode 1040 | Avg Reward 9.8\n",
      "Episode 1060 | Avg Reward 9.7\n",
      "Episode 1080 | Avg Reward 9.8\n",
      "Episode 1100 | Avg Reward 9.9\n",
      "Episode 1120 | Avg Reward 9.8\n",
      "Episode 1140 | Avg Reward 9.7\n",
      "Episode 1160 | Avg Reward 10.0\n",
      "Episode 1180 | Avg Reward 10.2\n",
      "Episode 1200 | Avg Reward 10.2\n",
      "Episode 1220 | Avg Reward 9.9\n",
      "Episode 1240 | Avg Reward 10.1\n",
      "Episode 1260 | Avg Reward 9.8\n",
      "Episode 1280 | Avg Reward 9.8\n",
      "Episode 1300 | Avg Reward 9.8\n",
      "Episode 1320 | Avg Reward 9.9\n",
      "Episode 1340 | Avg Reward 9.9\n",
      "Episode 1360 | Avg Reward 9.7\n",
      "Episode 1380 | Avg Reward 9.7\n",
      "Episode 1400 | Avg Reward 9.8\n",
      "Episode 1420 | Avg Reward 10.2\n",
      "Episode 1440 | Avg Reward 9.6\n",
      "Episode 1460 | Avg Reward 9.8\n",
      "Episode 1480 | Avg Reward 10.1\n",
      "Episode 1500 | Avg Reward 9.8\n",
      "Episode 1520 | Avg Reward 9.8\n",
      "Episode 1540 | Avg Reward 9.9\n",
      "Episode 1560 | Avg Reward 9.9\n",
      "Episode 1580 | Avg Reward 9.9\n",
      "Episode 1600 | Avg Reward 9.8\n",
      "Episode 1620 | Avg Reward 9.8\n",
      "Episode 1640 | Avg Reward 9.7\n",
      "Episode 1660 | Avg Reward 9.9\n",
      "Episode 1680 | Avg Reward 9.8\n",
      "Episode 1700 | Avg Reward 9.8\n",
      "Episode 1720 | Avg Reward 9.9\n",
      "Episode 1740 | Avg Reward 9.7\n",
      "Episode 1760 | Avg Reward 9.6\n",
      "Episode 1780 | Avg Reward 10.0\n",
      "Episode 1800 | Avg Reward 10.0\n",
      "Episode 1820 | Avg Reward 10.0\n",
      "Episode 1840 | Avg Reward 10.1\n",
      "Episode 1860 | Avg Reward 9.9\n",
      "Episode 1880 | Avg Reward 9.8\n",
      "Episode 1900 | Avg Reward 9.9\n",
      "Episode 1920 | Avg Reward 9.9\n",
      "Episode 1940 | Avg Reward 9.7\n",
      "Episode 1960 | Avg Reward 9.8\n",
      "Episode 1980 | Avg Reward 9.9\n",
      "Episode 2000 | Avg Reward 9.9\n",
      "Episode 2020 | Avg Reward 10.2\n",
      "Episode 2040 | Avg Reward 9.9\n",
      "Episode 2060 | Avg Reward 9.8\n",
      "Episode 2080 | Avg Reward 9.9\n",
      "Episode 2100 | Avg Reward 9.7\n",
      "Episode 2120 | Avg Reward 9.8\n",
      "Episode 2140 | Avg Reward 9.9\n",
      "Episode 2160 | Avg Reward 11.1\n",
      "Episode 2180 | Avg Reward 10.6\n",
      "Episode 2200 | Avg Reward 13.1\n",
      "Episode 2220 | Avg Reward 14.0\n",
      "Episode 2240 | Avg Reward 12.4\n",
      "Episode 2260 | Avg Reward 13.9\n",
      "Episode 2280 | Avg Reward 11.7\n",
      "Episode 2300 | Avg Reward 11.9\n",
      "Episode 2320 | Avg Reward 10.4\n",
      "Episode 2340 | Avg Reward 10.6\n",
      "Episode 2360 | Avg Reward 10.9\n",
      "Episode 2380 | Avg Reward 10.5\n",
      "Episode 2400 | Avg Reward 10.4\n",
      "Episode 2420 | Avg Reward 10.2\n",
      "Episode 2440 | Avg Reward 10.2\n",
      "Episode 2460 | Avg Reward 9.9\n",
      "Episode 2480 | Avg Reward 10.0\n",
      "Episode 2500 | Avg Reward 9.9\n",
      "Episode 2520 | Avg Reward 9.8\n",
      "Episode 2540 | Avg Reward 9.9\n",
      "Episode 2560 | Avg Reward 10.0\n",
      "Episode 2580 | Avg Reward 10.9\n",
      "Episode 2600 | Avg Reward 12.2\n",
      "Episode 2620 | Avg Reward 14.1\n",
      "Episode 2640 | Avg Reward 12.3\n",
      "Episode 2660 | Avg Reward 13.2\n",
      "Episode 2680 | Avg Reward 12.7\n",
      "Episode 2700 | Avg Reward 11.4\n",
      "Episode 2720 | Avg Reward 10.0\n",
      "Episode 2740 | Avg Reward 10.3\n",
      "Episode 2760 | Avg Reward 11.5\n",
      "Episode 2780 | Avg Reward 10.3\n",
      "Episode 2800 | Avg Reward 11.0\n",
      "Episode 2820 | Avg Reward 13.2\n",
      "Episode 2840 | Avg Reward 12.3\n",
      "Episode 2860 | Avg Reward 10.3\n",
      "Episode 2880 | Avg Reward 9.9\n",
      "Episode 2900 | Avg Reward 9.4\n",
      "Episode 2920 | Avg Reward 9.3\n",
      "Episode 2940 | Avg Reward 9.1\n",
      "Episode 2960 | Avg Reward 9.2\n",
      "Episode 2980 | Avg Reward 10.7\n",
      "Episode 3000 | Avg Reward 11.3\n",
      "Episode 3020 | Avg Reward 11.6\n",
      "Episode 3040 | Avg Reward 11.0\n",
      "Episode 3060 | Avg Reward 9.7\n",
      "Episode 3080 | Avg Reward 10.1\n",
      "Episode 3100 | Avg Reward 9.2\n",
      "Episode 3120 | Avg Reward 9.5\n",
      "Episode 3140 | Avg Reward 9.3\n",
      "Episode 3160 | Avg Reward 9.2\n",
      "Episode 3180 | Avg Reward 8.8\n",
      "Episode 3200 | Avg Reward 9.2\n",
      "Episode 3220 | Avg Reward 8.8\n",
      "Episode 3240 | Avg Reward 9.3\n",
      "Episode 3260 | Avg Reward 9.3\n",
      "Episode 3280 | Avg Reward 9.6\n",
      "Episode 3300 | Avg Reward 9.6\n",
      "Episode 3320 | Avg Reward 8.8\n",
      "Episode 3340 | Avg Reward 9.3\n",
      "Episode 3360 | Avg Reward 9.4\n",
      "Episode 3380 | Avg Reward 9.7\n",
      "Episode 3400 | Avg Reward 10.2\n",
      "Episode 3420 | Avg Reward 10.8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m returns \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(returns, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mDEVICE)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[43mppo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43macts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mact_log_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgaes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m ppo\u001b[38;5;241m.\u001b[39mtrain_value(obs, returns)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (episode_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m print_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[1;32mIn[28], line 29\u001b[0m, in \u001b[0;36mPPOTrainer.train_policy\u001b[1;34m(self, obs, acts, old_log_probs, gaes)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     28\u001b[0m new_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mac\u001b[38;5;241m.\u001b[39mpolicy(obs)\n\u001b[1;32m---> 29\u001b[0m new_logits \u001b[38;5;241m=\u001b[39m \u001b[43mCategorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_logits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m new_log_probs \u001b[38;5;241m=\u001b[39m new_logits\u001b[38;5;241m.\u001b[39mlog_prob(acts)\n\u001b[0;32m     32\u001b[0m policy_ratio \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(new_log_probs \u001b[38;5;241m-\u001b[39m old_log_probs)\n",
      "File \u001b[1;32mc:\\Users\\rohan\\Desktop\\175\\Tetris-AI\\myenv\\lib\\site-packages\\torch\\distributions\\categorical.py:66\u001b[0m, in \u001b[0;36mCategorical.__init__\u001b[1;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`logits` parameter must be at least one-dimensional.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# Normalize\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogits \u001b[38;5;241m=\u001b[39m logits \u001b[38;5;241m-\u001b[39m \u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogsumexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobs \u001b[38;5;28;01mif\u001b[39;00m probs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "ep_rewards = []\n",
    "for episode_idx in range(n_episodes):\n",
    "  # Perform rollout\n",
    "  train_data, reward = rollout(model, env)\n",
    "  ep_rewards.append(reward)\n",
    "\n",
    "  # Shuffle\n",
    "  permute_idxs = np.random.permutation(len(train_data[0]))\n",
    "\n",
    "  # Policy data\n",
    "  obs = torch.tensor(train_data[0][permute_idxs],\n",
    "                     dtype=torch.float32, device=DEVICE)\n",
    "  acts = torch.tensor(train_data[1][permute_idxs],\n",
    "                      dtype=torch.int32, device=DEVICE)\n",
    "  gaes = torch.tensor(train_data[3][permute_idxs],\n",
    "                      dtype=torch.float32, device=DEVICE)\n",
    "  gaes = (gaes - gaes.mean()) / (gaes.std() + 1e-8) # NORMALIZE\n",
    "  act_log_probs = torch.tensor(train_data[4][permute_idxs],\n",
    "                               dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "  # Value data\n",
    "  returns = discount_rewards(train_data[2])[permute_idxs]\n",
    "  returns = torch.tensor(returns, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "  # Train model\n",
    "  ppo.train_policy(obs, acts, act_log_probs, gaes)\n",
    "  ppo.train_value(obs, returns)\n",
    "\n",
    "  if (episode_idx + 1) % print_freq == 0:\n",
    "    print('Episode {} | Avg Reward {:.1f}'.format(\n",
    "        episode_idx + 1, np.mean(ep_rewards[-print_freq:])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
