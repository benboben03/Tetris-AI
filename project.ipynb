{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Our models take too long to train and run full games so we are just including our functions but not calling them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random, randint, sample\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "from collections import deque\n",
    "\n",
    "current_folder = os.getcwd()\n",
    "tetris_folder = os.path.join(current_folder, 'src','dqn','modified_tetris.py')\n",
    "sys.path.append(tetris_folder)\n",
    "from modified_tetris import Tetris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Sequential(nn.Linear(4, 64), nn.ReLU(inplace=True))\n",
    "        self.conv2 = nn.Sequential(nn.Linear(64, 64), nn.ReLU(inplace=True))\n",
    "        self.conv3 = nn.Sequential(nn.Linear(64, 1))\n",
    "\n",
    "        self._create_weights()\n",
    "\n",
    "    def _create_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training (not called)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "WIDTH = 10  # Width of board\n",
    "HEIGHT = 20  # Height of board\n",
    "BLOCK_SIZE = 30  # Block size when rendering\n",
    "BATCH_SIZE = 512  # High batch size\n",
    "LEARNING_RATE = 1e-3\n",
    "GAMMA = 0.99\n",
    "INITIAL_EPSILON = 1.0\n",
    "FINAL_EPSILON = 1e-3\n",
    "NUM_DECAY_EPOCHS = 1800\n",
    "NUM_EPOCHS = 3000\n",
    "SAVE_INTERVAL = 50\n",
    "REPLAY_MEMORY_SIZE = 28000\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train():\n",
    "    torch.manual_seed(42)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(42)\n",
    "\n",
    "    env = Tetris(width=WIDTH, height=HEIGHT, block_size=BLOCK_SIZE)\n",
    "    model = DQN().to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    state = env.reset().to(DEVICE)\n",
    "    replay_memory = deque(maxlen=REPLAY_MEMORY_SIZE)\n",
    "    epoch = 0\n",
    "\n",
    "    # Data for plotting\n",
    "    epoch_scores = []\n",
    "\n",
    "    while epoch < NUM_EPOCHS:\n",
    "        next_steps = env.get_next_states()\n",
    "        # Epsilon Greedy\n",
    "        epsilon = FINAL_EPSILON + (max(NUM_DECAY_EPOCHS - epoch, 0) * \n",
    "                                   (INITIAL_EPSILON - FINAL_EPSILON) / NUM_DECAY_EPOCHS)\n",
    "        next_actions, next_states = zip(*next_steps.items())\n",
    "        next_states = torch.stack(next_states).to(DEVICE)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = model(next_states)[:, 0]\n",
    "        model.train()\n",
    "\n",
    "        if random() <= epsilon:\n",
    "            index = randint(0, len(next_steps) - 1)\n",
    "        else:\n",
    "            index = torch.argmax(predictions).item()\n",
    "\n",
    "        next_state = next_states[index, :]\n",
    "        action = next_actions[index]\n",
    "\n",
    "        reward, done = env.step(action, render=False)\n",
    "        next_state = next_state.to(DEVICE)\n",
    "        replay_memory.append([state, reward, next_state, done])\n",
    "\n",
    "        if done:\n",
    "            final_score = env.score\n",
    "            final_tetrominoes = env.tetrominoes\n",
    "            final_cleared_lines = env.cleared_lines\n",
    "            state = env.reset().to(DEVICE)\n",
    "\n",
    "            # Store score for plotting\n",
    "            epoch_scores.append(final_score)\n",
    "        else:\n",
    "            state = next_state\n",
    "            continue\n",
    "\n",
    "        if len(replay_memory) < REPLAY_MEMORY_SIZE / 10:\n",
    "            continue\n",
    "\n",
    "        epoch += 1\n",
    "        batch = sample(replay_memory, min(len(replay_memory), BATCH_SIZE))\n",
    "        state_batch, reward_batch, next_state_batch, done_batch = zip(*batch)\n",
    "        state_batch = torch.stack(tuple(state for state in state_batch)).to(DEVICE)\n",
    "        reward_batch = torch.from_numpy(np.array(reward_batch, dtype=np.float32)[:, None]).to(DEVICE)\n",
    "        next_state_batch = torch.stack(tuple(state for state in next_state_batch)).to(DEVICE)\n",
    "\n",
    "        q_values = model(state_batch)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            next_prediction_batch = model(next_state_batch)\n",
    "        model.train()\n",
    "\n",
    "        # Compute the target Q-values for each transition\n",
    "        y_values = [\n",
    "            reward if done else reward + GAMMA * prediction\n",
    "            for reward, done, prediction in zip(reward_batch, done_batch, next_prediction_batch)\n",
    "        ]\n",
    "        y_tensor = torch.tensor(y_values, dtype=torch.float32, device=DEVICE)\n",
    "        y_batch = y_tensor[:, None]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(q_values, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(\"Epoch: {}/{}, Action: {}, Score: {}, Tetrominoes {}, Cleared lines: {}\".format(\n",
    "            epoch,\n",
    "            NUM_EPOCHS,\n",
    "            action,\n",
    "            final_score,\n",
    "            final_tetrominoes,\n",
    "            final_cleared_lines))\n",
    "\n",
    "        if epoch > 0 and epoch % SAVE_INTERVAL == 0:\n",
    "            torch.save(model.state_dict(), \"saved_model.pth\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"saved_model.pth\")\n",
    "\n",
    "    # Plotting the scores\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(len(epoch_scores)), epoch_scores, label=\"Score\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.title(\"Score vs. Epochs\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation (not called)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FPS = 300\n",
    "def evaluate_model(model, num_games=10):\n",
    "    model.eval()\n",
    "    env = Tetris(width=10, height=20, block_size=30)\n",
    "\n",
    "    total_score = 0\n",
    "    total_tetrominoes = 0\n",
    "    total_lines_cleared = 0\n",
    "\n",
    "    for game in range(num_games):\n",
    "        _ = env.reset().to(DEVICE)\n",
    "        game_score = 0\n",
    "        game_tetrominoes = 0\n",
    "        game_lines_cleared = 0\n",
    "\n",
    "        while True:\n",
    "            next_steps = env.get_next_states()\n",
    "            next_actions, next_states = zip(*next_steps.items())\n",
    "            next_states = torch.stack(next_states).to(DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                predictions = model(next_states)[:, 0]\n",
    "            best_action_index = torch.argmax(predictions).item()\n",
    "            action = next_actions[best_action_index]\n",
    "\n",
    "            _, done = env.step(action,render=False)\n",
    "\n",
    "            game_score = env.score\n",
    "            game_tetrominoes = env.tetrominoes\n",
    "            game_lines_cleared = env.cleared_lines\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Accumulate totals\n",
    "        total_score += game_score\n",
    "        total_tetrominoes += game_tetrominoes\n",
    "        total_lines_cleared += game_lines_cleared\n",
    "\n",
    "        print(f\"Game {game + 1}/{num_games} - Score: {game_score}, Tetrominoes: {game_tetrominoes}, Lines Cleared: {game_lines_cleared}\")\n",
    "\n",
    "    # Calculate averages\n",
    "    avg_score = total_score / num_games\n",
    "    avg_tetrominoes = total_tetrominoes / num_games\n",
    "    avg_lines_cleared = total_lines_cleared / num_games\n",
    "\n",
    "    print(f\"\\nEvaluation Results:\")\n",
    "    print(f\"Average Score: {avg_score}\")\n",
    "    print(f\"Average Tetrominoes: {avg_tetrominoes}\")\n",
    "    print(f\"Average Lines Cleared: {avg_lines_cleared}\")\n",
    "\n",
    "    return avg_score, avg_tetrominoes, avg_lines_cleared\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to evaluate our best DQN model, uncomment the last line in the following cell. It will take more than one minute though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_folder = os.getcwd()\n",
    "saved_model = os.path.join(current_folder, 'src', 'dqn', 'Trained_Models','adaptation2.pth')\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent_tetris = DQN().to(DEVICE)\n",
    "agent_tetris.load_state_dict(torch.load(saved_model))\n",
    "#avg_score, avg_tetrominoes, avg_lines_cleared = evaluate_model(agent_tetris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "current_folder = os.getcwd()\n",
    "video_path =  os.path.join(current_folder, 'src','dqn','dqn_model.mp4')\n",
    "\n",
    "HTML(f\"\"\"\n",
    "<video width=\"600\" controls>\n",
    "  <source src=\"{video_path}\" type=\"video/mp4\">\n",
    "  Your browser does not support the video tag.\n",
    "</video>\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
